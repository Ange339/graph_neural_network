{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aded2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vJMG8gbuQ5g2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25729,
     "status": "ok",
     "timestamp": 1755874642037,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "vJMG8gbuQ5g2",
    "outputId": "ce3cc13e-c2c9-4ae4-ad9b-920d3d1dbce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: igraph in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (0.11.9)\n",
      "Requirement already satisfied: texttable>=1.6.2 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from igraph) (1.7.0)\n",
      "Collecting scikit-network\n",
      "  Downloading scikit_network-0.33.3-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from scikit-network) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from scikit-network) (1.15.3)\n",
      "Downloading scikit_network-0.33.3-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 2.1/2.7 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-network\n",
      "Successfully installed scikit-network-0.33.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install igraph\n",
    "# !pip install scikit-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471eb8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torchmetrics) (2.1.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (78.1.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.14.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
      "   ---------------------------------------- 0.0/983.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 983.0/983.0 kB 48.1 MB/s  0:00:00\n",
      "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "\n",
      "   ---------------------------------------- 0/2 [lightning-utilities]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   -------------------- ------------------- 1/2 [torchmetrics]\n",
      "   ---------------------------------------- 2/2 [torchmetrics]\n",
      "\n",
      "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#!conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9247b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "\n",
    "# srs_url = f\"https://pytorch-geometric.com/whl/torch-{torch.__version__}.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3843554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
      "Collecting torch_scatter\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_scatter-2.1.2%2Bpt21cpu-cp310-cp310-win_amd64.whl (336 kB)\n",
      "Installing collected packages: torch_scatter\n",
      "Successfully installed torch_scatter-2.1.2+pt21cpu\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
      "Collecting torch_sparse\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_sparse-0.6.18%2Bpt21cpu-cp310-cp310-win_amd64.whl (788 kB)\n",
      "     ---------------------------------------- 0.0/788.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 788.9/788.9 kB 17.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_sparse) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from scipy->torch_sparse) (2.0.1)\n",
      "Installing collected packages: torch_sparse\n",
      "Successfully installed torch_sparse-0.6.18+pt21cpu\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
      "Collecting torch_cluster\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_cluster-1.6.3%2Bpt21cpu-cp310-cp310-win_amd64.whl (506 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_cluster) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from scipy->torch_cluster) (2.0.1)\n",
      "Installing collected packages: torch_cluster\n",
      "Successfully installed torch_cluster-1.6.3+pt21cpu\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.1.0+cpu.html\n",
      "Collecting torch_spline_conv\n",
      "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt21cpu-cp310-cp310-win_amd64.whl (177 kB)\n",
      "Installing collected packages: torch_spline_conv\n",
      "Successfully installed torch_spline_conv-1.2.2+pt21cpu\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch_scatter -f $srs_url\n",
    "# !pip install torch_sparse -f $srs_url\n",
    "# !pip install torch_cluster -f $srs_url\n",
    "# !pip install torch_spline_conv -f $srs_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c07ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting fsspec (from torch_geometric)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_geometric) (2.0.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_geometric) (7.0.0)\n",
      "Collecting pyparsing (from torch_geometric)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from torch_geometric) (2.32.4)\n",
      "Collecting tqdm (from torch_geometric)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp->torch_geometric)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp->torch_geometric)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->torch_geometric)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->torch_geometric)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->torch_geometric)\n",
      "  Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->torch_geometric)\n",
      "  Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->torch_geometric) (3.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from requests->torch_geometric) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\angel\\anaconda3\\envs\\pyg\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Using cached torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
      "Downloading aiohttp-3.12.15-cp310-cp310-win_amd64.whl (452 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Downloading yarl-1.20.1-cp310-cp310-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp310-cp310-win_amd64.whl (43 kB)\n",
      "Downloading propcache-0.3.2-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, pyparsing, propcache, multidict, fsspec, frozenlist, attrs, async-timeout, aiohappyeyeballs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ------ ---------------------------------  2/13 [propcache]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [frozenlist]\n",
      "   --------------------- ------------------  7/13 [async-timeout]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ------------------------------------ --- 12/13 [torch_geometric]\n",
      "   ---------------------------------------- 13/13 [torch_geometric]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 frozenlist-1.7.0 fsspec-2025.7.0 multidict-6.6.4 propcache-0.3.2 pyparsing-3.2.3 torch_geometric-2.6.1 tqdm-4.67.1 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Voe3ED3WRFsV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53284,
     "status": "ok",
     "timestamp": 1755874695512,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "Voe3ED3WRFsV",
    "outputId": "9b8f371c-ce89-4da5-9165-484c8e266a9f"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "\n",
    "# if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#   !pip install torch==2.4.0\n",
    "\n",
    "# # Install torch geometric\n",
    "# if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "#   torch_version = str(torch.__version__)\n",
    "#   scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "#   sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
    "#   !pip install torch-scatter -f $scatter_src\n",
    "#   !pip install torch-sparse -f $sparse_src\n",
    "#   !pip install torch-geometric\n",
    "#   !pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e675cc97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3286,
     "status": "ok",
     "timestamp": 1755874723445,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "e675cc97",
    "outputId": "7634e6dd-140e-4f9e-dcf4-2a361ca3547c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import scipy.sparse as sp\n",
    "from typing import Literal\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "#%env NX_CUGRAPH_AUTOCONFIG=True\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite, community\n",
    "nx.config.warnings_to_ignore.add(\"cache\")\n",
    "\n",
    "import igraph as ig\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, VGAE, to_hetero\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler  import NegativeSampling\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from torch_scatter import scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b81b1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  4.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2., 3., 4., 5., 6.],\n",
    "                   [7., 8., 9., 10., 11., 12.]])   # values\n",
    "group = torch.tensor([0, 0, 1, 1, 1, 0])     # group ids\n",
    "out = scatter_mean(x, group, dim=1)          # mean per group\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ea2770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Load parquet datasets\")\n",
    "parser.add_argument(\"--config\", type=str, default=\"config.yaml\", help=\"Path to YAML config file\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Load YAML\n",
    "with open(args.config, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2094a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_processing():\n",
    "    \"\"\"\n",
    "    Load and preprocess the data for the graph neural network.\n",
    "    An important step is the reindexing of user and book IDs after the filtering process.\n",
    "    \"\"\"\n",
    "    # Load files\n",
    "    users = pl.read_parquet(os.path.join(DIR, user_filename))\n",
    "    books = pl.read_parquet(os.path.join(DIR, book_filename))\n",
    "    interactions = pl.read_parquet(os.path.join(DIR, interactions_filename))\n",
    "    embeddings_descriptions = pl.read_parquet(os.path.join(DIR, embeddings_descriptions_filename))\n",
    "    embeddings_reviews = pl.read_parquet(os.path.join(DIR, embeddings_reviews_filename))\n",
    "    \n",
    "    # Drop hashed ID columns since we're going to redefine it\n",
    "    users = users.drop('user_id_hashed', strict=False)\n",
    "    books = books.drop('book_id_hashed', strict=False)\n",
    "    interactions = interactions.drop(['user_id_hashed', 'book_id_hashed'], strict=False)\n",
    "\n",
    "    print(f\"Initial size. Users: {len(users)}, Books: {len(books)}, Total: {len(users) + len(books)}, Edges: {len(interactions)}, Density: {len(interactions) / (len(users) * len(books)):4%}\")\n",
    "    users = users.filter(pl.col('review_coreness') >= review_coreness_k)\n",
    "    users = users.with_columns((pl.col('user_id').rank()-1).cast(pl.Int64).alias('user_id_hashed'))\n",
    "    books = books.filter(pl.col('review_coreness') >= review_coreness_k)\n",
    "    books = books.with_columns((pl.col('book_id').rank()-1).cast(pl.Int64).alias('book_id_hashed'))\n",
    "\n",
    "    interactions = interactions.join(users.select([\"user_id\", \"user_id_hashed\"]), on=\"user_id\", how=\"inner\")\n",
    "    interactions = interactions.join(books.select([\"book_id\", \"book_id_hashed\"]), on=\"book_id\", how=\"inner\")\n",
    "\n",
    "    print(f\"Filtered size. Users: {len(users)}, Books: {len(books)}, Total: {len(users) + len(books)}, Edges: {len(interactions)}, Density: {len(interactions) / (len(users) * len(books)):4%}\")\n",
    "\n",
    "    embeddings_descriptions = embeddings_descriptions.join(books.select([\"book_id\", \"book_id_hashed\"]), on=\"book_id\", how=\"inner\")\n",
    "    embeddings_reviews = embeddings_reviews.join(interactions.select([\"book_id\", \"user_id\", \"user_id_hashed\", \"book_id_hashed\"]), on=[\"book_id\", \"user_id\"], how=\"inner\")\n",
    "\n",
    "    ## Sorting\n",
    "    users = users.sort('user_id_hashed')\n",
    "    books = books.sort('book_id_hashed')\n",
    "    interactions = interactions.sort('user_id_hashed', 'book_id_hashed')\n",
    "    embeddings_descriptions = embeddings_descriptions.sort('book_id_hashed')\n",
    "    embeddings_reviews = embeddings_reviews.sort('user_id_hashed', 'book_id_hashed')\n",
    "\n",
    "    data = {\n",
    "        \"users\": users,\n",
    "        \"books\": books,\n",
    "        \"interactions\": interactions,\n",
    "        \"embeddings_descriptions\": embeddings_descriptions,\n",
    "        \"embeddings_reviews\": embeddings_reviews\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def graph_edges(data):\n",
    "    edges = data['interactions'].select(['user_id_hashed', 'book_id_hashed']).rows()\n",
    "    src, dst = [], []\n",
    "    for e in edges:\n",
    "        s, t = e\n",
    "        src.append(s)\n",
    "        dst.append(t)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def graph_node_random_features(data, n_nodes, dim):\n",
    "    \"Generate random features for graph nodes.\"\n",
    "    random_features = nn.Embedding(n_nodes, dim)\n",
    "    return random_features\n",
    "\n",
    "\n",
    "def graph_node_topological_features(data, feature: List[str], degree_log_transform=False):\n",
    "    \"Generate topological features for graph nodes.\"\n",
    "    if \"degree\" in feature and degree_log_transform:\n",
    "        data['users'] = data['users'].with_columns(\n",
    "            (pl.col('degree').log()).alias('degree')\n",
    "        )\n",
    "        data['books'] = data['books'].with_columns(\n",
    "            (pl.col('degree').log()).alias('degree')\n",
    "        )\n",
    "\n",
    "    users = data['users'].sort('user_id_hashed')\n",
    "    users_features = users.select(feature).to_numpy().T\n",
    "    books = data['books'].sort('book_id_hashed')\n",
    "    books_features = books.select(feature).to_numpy().T\n",
    "\n",
    "    users_feature = torch.tensor(users_features, dtype=torch.float32)\n",
    "    books_feature = torch.tensor(books_features, dtype=torch.float32)\n",
    "\n",
    "    return users_feature, books_feature\n",
    "\n",
    "\n",
    "def graph_one_hot_encoding(n_nodes):\n",
    "    \"Generate one-hot encoding for graph nodes. Not Recommended\"\n",
    "    return torch.eye(n_nodes)\n",
    "\n",
    "\n",
    "def embeddings_parquet2torch(data):\n",
    "    \"\"\"\n",
    "    Convert embeddings from parquet to torch tensors.\n",
    "    Embeddings are stored as row, to_numpy operation take the column as numpy row, so we need to transpose them.\n",
    "    \"\"\"\n",
    "\n",
    "    emb_rev_columns = [col for col in data['embeddings_reviews'].columns if col.startswith('column')]\n",
    "    embeddings_reviews = data['embeddings_reviews'].select(emb_rev_columns)\n",
    "    embeddings_reviews = embeddings_reviews.to_numpy().T\n",
    "    embeddings_reviews = torch.tensor(embeddings_reviews, dtype=torch.float32)\n",
    "\n",
    "    emb_des_columns = [col for col in data['embeddings_descriptions'].columns if col.startswith('column')]\n",
    "    embeddings_descriptions = data['embeddings_descriptions'].select(emb_des_columns)\n",
    "    embeddings_descriptions = embeddings_descriptions.to_numpy().T\n",
    "    embeddings_descriptions = torch.tensor(embeddings_descriptions, dtype=torch.float32)\n",
    "\n",
    "    assert embeddings_reviews.shape[1] == data['embeddings_reviews'].shape[0] and embeddings_reviews.shape[0] == len(emb_rev_columns), \"Mismatch in embeddings_reviews shape\"\n",
    "    assert embeddings_descriptions.shape[1] == data['embeddings_descriptions'].shape[0] and embeddings_descriptions.shape[0] == len(emb_des_columns), \"Mismatch in embeddings_descriptions shape\"\n",
    "\n",
    "    return embeddings_reviews, embeddings_descriptions\n",
    "\n",
    "\n",
    "def graph_node_textual_feature_description(data, embeddings_descriptions):\n",
    "    \"Since the reviews are unique, return the embeddings for the book descriptions.\"\n",
    "    return embeddings_descriptions\n",
    "\n",
    "\n",
    "def graph_node_textual_features_reviews_aggr(data, embeddings_reviews, aggr_fn: Literal[\"mean\", \"sum\"] = 'mean'):\n",
    "    \"Return the aggregated user review embeddings.\"\n",
    "    VALID_AGGR = {\"mean\", \"sum\"}\n",
    "    if aggr_fn not in VALID_AGGR:\n",
    "        raise ValueError(f\"aggr_fn must be 'mean' or 'sum', got '{aggr_fn}'\")\n",
    "\n",
    "    user_ids = data['embeddings_reviews']['user_id_hashed'].to_numpy()\n",
    "    user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "\n",
    "    book_ids = data['embeddings_reviews']['book_id_hashed'].to_numpy()\n",
    "    book_ids = torch.tensor(book_ids, dtype=torch.long)\n",
    "\n",
    "    if aggr_fn == \"mean\":\n",
    "        user_review_embeddings_aggr = scatter_mean(embeddings_reviews, user_ids, dim=1)\n",
    "        book_review_embeddings_aggr = scatter_mean(embeddings_reviews, book_ids, dim=1)\n",
    "    elif aggr_fn == \"sum\":\n",
    "        user_review_embeddings_aggr = scatter_sum(embeddings_reviews, user_ids, dim=1)\n",
    "        book_review_embeddings_aggr = scatter_sum(embeddings_reviews, book_ids, dim=1)\n",
    "\n",
    "    return user_review_embeddings_aggr, book_review_embeddings_aggr\n",
    "\n",
    "\n",
    "def one_strategy(data):\n",
    "    \"One strategy for generating graph node features.\"\n",
    "    # Topological Features\n",
    "    users_topo_feature, books_topo_feature = graph_node_topological_features(data, feature=[\"degree\"], degree_log_transform=True)\n",
    "    \n",
    "    # Textual Features\n",
    "    embeddings_reviews, embeddings_descriptions = embeddings_parquet2torch(data)\n",
    "    books_textual_feature = graph_node_textual_feature_description(data, embeddings_descriptions)\n",
    "    user_review_embeddings_aggr, book_review_embeddings_aggr = graph_node_textual_features_reviews_aggr(data, embeddings_reviews, aggr_fn='mean')\n",
    "\n",
    "    user_x = torch.cat([users_topo_feature, user_review_embeddings_aggr], dim=1)\n",
    "    book_x = torch.cat([books_topo_feature, books_textual_feature, book_review_embeddings_aggr], dim=1)\n",
    "\n",
    "    return user_x, book_x\n",
    "\n",
    "def second_strategy(data, dimensions):\n",
    "    # Topological Features\n",
    "    users_topo_feature, books_topo_feature = graph_node_topological_features(data, feature=[\"degree\"], degree_log_transform=True)\n",
    "\n",
    "    # Random Features\n",
    "    user_n = data['users'].shape[0]\n",
    "    book_n = data['books'].shape[0]\n",
    "    users_random_feature = torch.randn(user_n, dimensions)\n",
    "    books_random_feature = torch.randn(book_n, dimensions)\n",
    "\n",
    "    user_x = torch.cat([users_topo_feature, users_random_feature], dim=1)\n",
    "    book_x = torch.cat([books_topo_feature, books_random_feature], dim=1)\n",
    "\n",
    "    return user_x, book_x\n",
    "\n",
    "def third_strategy(data, dimensions):\n",
    "    user_n = data['users'].shape[0]\n",
    "    book_n = data['books'].shape[0]\n",
    "    user_x = torch.randn(user_n, dimensions)\n",
    "    book_x = torch.randn(book_n, dimensions)\n",
    "\n",
    "    return user_x, book_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce506cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.cfg = config\n",
    "    \n",
    "    def load(self):\n",
    "        \"Load the files by reading the filenames in config\"\n",
    "        data_cfg = self.cfg['data']\n",
    "        DIR = data_cfg['dir']\n",
    "        users = pl.read_parquet(os.path.join(DIR, data_cfg['users_filename']))\n",
    "        books = pl.read_parquet(os.path.join(DIR, data_cfg['books_filename']))\n",
    "        interactions = pl.read_parquet(os.path.join(DIR, data_cfg['interactions_filename']))\n",
    "        descriptions = pl.read_parquet(os.path.join(DIR, data_cfg['descriptions_filename']), columns=[\"book_id\", \"filtered\"])\n",
    "        reviews = pl.read_parquet(os.path.join(DIR, data_cfg['reviews_filename']), columns=[\"user_id\", \"book_id\", \"filtered\"])\n",
    "        embeddings_descriptions = pl.read_parquet(os.path.join(DIR, data_cfg['embeddings_descriptions_filename']))\n",
    "        embeddings_reviews = pl.read_parquet(os.path.join(DIR, data_cfg['embeddings_reviews_filename']))\n",
    "        data =  {\n",
    "            \"users\": users,\n",
    "            \"books\": books,\n",
    "            \"interactions\": interactions,\n",
    "            \"descriptions\": descriptions,\n",
    "            \"reviews\": reviews,\n",
    "            \"embeddings_descriptions\": embeddings_descriptions,\n",
    "            \"embeddings_reviews\": embeddings_reviews\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def preprocess(self, data, method: Literal[\"coreness\", \"review_coreness\"] = 'review_coreness'):\n",
    "        \"Preprocess the loaded data by eliminating observations based on the method\"\n",
    "        pp_cfg = self.cfg['processing']\n",
    "\n",
    "        users = data[\"users\"]\n",
    "        books = data[\"books\"]\n",
    "        interactions = data[\"interactions\"]\n",
    "        descriptions = data[\"descriptions\"]\n",
    "        reviews = data[\"reviews\"]\n",
    "        embeddings_descriptions = data[\"embeddings_descriptions\"]\n",
    "        embeddings_reviews = data[\"embeddings_reviews\"]\n",
    "\n",
    "\n",
    "        # Drop hashed ID columns since we're going to redefine it\n",
    "        users = users.drop('user_id_hashed', strict=False)\n",
    "        books = books.drop('book_id_hashed', strict=False)\n",
    "        interactions = interactions.drop(['user_id_hashed', 'book_id_hashed'], strict=False)\n",
    "\n",
    "        print(f\"Initial size. Users: {len(users)}, Books: {len(books)}, Total: {len(users) + len(books)}, Edges: {len(interactions)}, Density: {len(interactions) / (len(users) * len(books)):4%}\")\n",
    "        \n",
    "        # Filter by degree. \"review_coreness\" should be always used\n",
    "        \n",
    "        if method == \"coreness\":\n",
    "            # Apply coreness filtering\n",
    "            coreness_k = pp_cfg.get(\"coreness_k\", 3)\n",
    "            users = users.filter(pl.col('coreness') >= coreness_k)\n",
    "            books = books.filter(pl.col('coreness') >= coreness_k)\n",
    "        elif method == \"review_coreness\":\n",
    "            # Apply review coreness filtering\n",
    "            review_coreness_k = pp_cfg.get(\"review_coreness_k\", 3)\n",
    "            users = users.filter(pl.col('review_coreness') >= review_coreness_k)\n",
    "            books = books.filter(pl.col('review_coreness') >= review_coreness_k)\n",
    "\n",
    "        ## Reindexing\n",
    "        users = users.with_columns((pl.col('user_id').rank()-1).cast(pl.Int64).alias('user_id_hashed'))\n",
    "        books = books.with_columns((pl.col('book_id').rank()-1).cast(pl.Int64).alias('book_id_hashed'))\n",
    "\n",
    "        ## Filter and add the new_index information for the rest of data\n",
    "        interactions = interactions.join(users.select([\"user_id\", \"user_id_hashed\"]), on=\"user_id\", how=\"inner\")\n",
    "        interactions = interactions.join(books.select([\"book_id\", \"book_id_hashed\"]), on=\"book_id\", how=\"inner\") # Drop rows with reviews that are not english\n",
    "\n",
    "        print(f\"Filtered size. Users: {len(users)}, Books: {len(books)}, Total: {len(users) + len(books)}, Edges: {len(interactions)}, Density: {len(interactions) / (len(users) * len(books)):4%}\")\n",
    "\n",
    "        embeddings_descriptions = embeddings_descriptions.join(books.select([\"book_id\", \"book_id_hashed\"]), on=\"book_id\", how=\"inner\")\n",
    "        embeddings_reviews = embeddings_reviews.join(interactions.select([\"book_id\", \"user_id\", \"user_id_hashed\", \"book_id_hashed\"]), on=[\"book_id\", \"user_id\"], how=\"inner\")\n",
    "\n",
    "        ## Filter also the embeddings\n",
    "        print(f\"Initial textual embeddings size. Descriptions: {len(embeddings_descriptions)}, Reviews: {len(embeddings_reviews)}, Total: {len(embeddings_descriptions) + len(embeddings_reviews)}\")\n",
    "        descriptions = data['descriptions'].filter(pl.col(\"filtered\") == 1).drop(\"filtered\")\n",
    "        embeddings_descriptions = embeddings_descriptions.join(descriptions[[\"book_id\"]], on=\"book_id\", how=\"inner\")\n",
    "\n",
    "        reviews = data['reviews'].filter(pl.col(\"filtered\") == 1).drop(\"filtered\")\n",
    "        embeddings_reviews = embeddings_reviews.join(reviews[[\"user_id\", \"book_id\"]], on=[\"user_id\", \"book_id\"], how=\"inner\")\n",
    "        print(f\"Filtered textual embeddings size. Descriptions: {len(embeddings_descriptions)}, Reviews: {len(embeddings_reviews)}, Total: {len(embeddings_descriptions) + len(embeddings_reviews)}\")\n",
    "\n",
    "        ## Sorting\n",
    "        users = users.sort('user_id_hashed')\n",
    "        books = books.sort('book_id_hashed')\n",
    "        interactions = interactions.sort('user_id_hashed', 'book_id_hashed')\n",
    "        embeddings_descriptions = embeddings_descriptions.sort('book_id_hashed')\n",
    "        embeddings_reviews = embeddings_reviews.sort('user_id_hashed', 'book_id_hashed')\n",
    "\n",
    "        data = {\n",
    "            \"users\": users,\n",
    "            \"books\": books,\n",
    "            \"interactions\": interactions,\n",
    "            \"embeddings_descriptions\": embeddings_descriptions,\n",
    "            \"embeddings_reviews\": embeddings_reviews\n",
    "        }\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "285557d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbeddingProcessor:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg # It won't be used\n",
    "\n",
    "    def to_torch(self, data):\n",
    "        if  isinstance(data['embeddings_reviews'], pl.DataFrame):\n",
    "            return self.embeddings_parquet2torch(data)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def embeddings_parquet2torch(self, data):\n",
    "        \"\"\"\n",
    "        Convert embeddings from parquet to torch tensors.\n",
    "        Return also the index list for each embeddings \n",
    "        \"\"\"\n",
    "\n",
    "        # Reviews\n",
    "        reviews = data['embeddings_reviews']\n",
    "\n",
    "        ## Extract the embeddings\n",
    "        emb_rev_columns = [col for col in reviews.columns if col.startswith('column')]\n",
    "        embeddings_reviews = reviews.select(emb_rev_columns)\n",
    "        embeddings_reviews = embeddings_reviews.to_numpy()\n",
    "        embeddings_reviews = torch.tensor(embeddings_reviews, dtype=torch.float32)\n",
    "\n",
    "        ## Extract the ids\n",
    "        users_review_id = reviews['user_id_hashed'].to_numpy()\n",
    "        users_review_id = torch.tensor(users_review_id, dtype=torch.long)\n",
    "        books_review_id = reviews['book_id_hashed'].to_numpy()\n",
    "        books_review_id = torch.tensor(books_review_id, dtype=torch.long)\n",
    "\n",
    "        # Descriptions\n",
    "        descriptions = data['embeddings_descriptions']\n",
    "\n",
    "        ## Extract the embeddings\n",
    "        emb_des_columns = [col for col in descriptions.columns if col.startswith('column')]\n",
    "        embeddings_descriptions = descriptions.select(emb_des_columns)\n",
    "        embeddings_descriptions = embeddings_descriptions.to_numpy()\n",
    "        embeddings_descriptions = torch.tensor(embeddings_descriptions, dtype=torch.float32)\n",
    "\n",
    "        books_des_id = descriptions['book_id_hashed'].to_numpy()\n",
    "        books_des_id = torch.tensor(books_des_id, dtype=torch.long)\n",
    "\n",
    "        assert embeddings_reviews.shape[0] == reviews.shape[0] and embeddings_reviews.shape[1] == len(emb_rev_columns), \"Mismatch in embeddings_reviews shape\"\n",
    "        assert embeddings_descriptions.shape[0] == descriptions.shape[0] and embeddings_descriptions.shape[1] == len(emb_des_columns), \"Mismatch in embeddings_descriptions shape\"\n",
    "\n",
    "        # Overwrite to free the memory\n",
    "        data['embeddings_reviews'] = embeddings_reviews\n",
    "        data['embeddings_descriptions'] = embeddings_descriptions\n",
    "        data['users_review_id'] = users_review_id\n",
    "        data['books_review_id'] = books_review_id\n",
    "        data['books_des_id'] = books_des_id\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(ABC):\n",
    "    \"\"\"Abstract base class for all feature extractors.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self, data, node_type: str) -> torch.Tensor:\n",
    "        \"\"\"Each extractor must implement this to return a feature tensor.\"\"\"\n",
    "        pass\n",
    "\n",
    "class TopologicalFeatures(FeatureExtractor):\n",
    "    def __init__(self, features: List[str], degree_log_transform: bool = False):\n",
    "        self.features = features\n",
    "        self.degree_log_transform = degree_log_transform\n",
    "\n",
    "    def build(self, data, node_type: Literal[\"users\", \"books\"]) -> torch.Tensor:\n",
    "        df = data[node_type]  # \"users\" or \"books\"\n",
    "\n",
    "        if \"degree\" in self.features and self.degree_log_transform:\n",
    "            df = df.with_columns((pl.col(\"degree\").log()).alias(\"degree\"))\n",
    "\n",
    "        feats = df.select(self.features).to_numpy()\n",
    "        return torch.tensor(feats, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class OneHotEncodedFeatures(FeatureExtractor):\n",
    "    def build(self, data, node_type):\n",
    "        \"Generate one-hot encoding for graph nodes. Not Recommended\"\n",
    "        n_nodes = data[node_type].shape[0]\n",
    "        return torch.eye(n_nodes)\n",
    "\n",
    "\n",
    "class RandomFeatures(FeatureExtractor):\n",
    "    def __init__(self, dim: int):\n",
    "        self.dim = dim\n",
    "\n",
    "    def build(self, data, node_type: Literal[\"users\", \"books\"]) -> torch.Tensor:\n",
    "        num_nodes = data[node_type].shape[0]\n",
    "        return nn.Embedding(num_nodes, self.dim)\n",
    "\n",
    "\n",
    "class TextualDescriptionFeatures(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def build(self, data, node_type: Literal[\"users\", \"books\"]) -> torch.Tensor:\n",
    "        if node_type == \"users\":\n",
    "            print(f\"Users do not have descriptions\")\n",
    "            return None\n",
    "        else:\n",
    "            ids = data['books_des_id']\n",
    "            embeddings = data['embeddings_descriptions']\n",
    "            perm = torch.argsort(ids)\n",
    "            embeddings = embeddings[perm]\n",
    "            return embeddings\n",
    "\n",
    "class TextualReviewFeatures(FeatureExtractor):\n",
    "    def __init__(self, aggr_fn: Literal[\"mean\", \"sum\"] = 'mean'):\n",
    "        VALID_AGGR = {\"mean\", \"sum\"}\n",
    "        if aggr_fn not in VALID_AGGR:\n",
    "            raise ValueError(f\"aggr_fn must be 'mean' or 'sum', got '{aggr_fn}'\")\n",
    "        self.aggr_fn = aggr_fn\n",
    "\n",
    "    def build(self, data, node_type: Literal[\"users\", \"books\"]) -> torch.Tensor:\n",
    "\n",
    "        ids = data[f'{node_type}_review_id']  # 'users_review_id' or 'books_review_id'\n",
    "        embeddings_reviews = data['embeddings_reviews']\n",
    "\n",
    "        if self.aggr_fn == \"mean\":\n",
    "            embeddings_reviews_aggr = scatter_mean(embeddings_reviews, ids, dim=0)\n",
    "        elif self.aggr_fn == \"sum\":\n",
    "            embeddings_reviews_aggr = scatter_sum(embeddings_reviews, ids, dim=0)\n",
    "\n",
    "        return embeddings_reviews_aggr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca2e8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_REGISTRY = {\n",
    "    \"topo\": TopologicalFeatures,\n",
    "    \"random\": RandomFeatures,\n",
    "    \"textual_desc\": TextualDescriptionFeatures,\n",
    "    \"textual_reviews\": TextualReviewFeatures,\n",
    "}\n",
    "\n",
    "\n",
    "class GraphLoader():\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def build_edges(self, data):\n",
    "        \"Build graph edges from the data.\"\n",
    "        edges = data['interactions'].select(['user_id_hashed', 'book_id_hashed']).rows()\n",
    "        src, dst = [], []\n",
    "        for e in edges:\n",
    "            s, t = e\n",
    "            src.append(s)\n",
    "            dst.append(t)\n",
    "        edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "        return edge_index\n",
    "\n",
    "    def build_nodes_features(self, data):\n",
    "        \"\"\"Return user_x, book_x feature tensors.\"\"\"\n",
    "        user_feats, book_feats = [], []\n",
    "\n",
    "        for fconf in self.cfg.get(\"user\", []):\n",
    "            extractor_cls = FEATURE_REGISTRY[fconf[\"name\"]]\n",
    "            extractor = extractor_cls(**{k: v for k, v in fconf.items() if k != \"name\"})\n",
    "            feat = extractor.build(data, \"users\")\n",
    "            if feat is not None:\n",
    "                user_feats.append(feat)\n",
    "\n",
    "        for fconf in self.cfg.get(\"book\", []):\n",
    "            extractor_cls = FEATURE_REGISTRY[fconf[\"name\"]]\n",
    "            extractor = extractor_cls(**{k: v for k, v in fconf.items() if k != \"name\"})\n",
    "            feat = extractor.build(data, \"books\")\n",
    "            if feat is not None:\n",
    "                book_feats.append(feat)\n",
    "\n",
    "        user_x = torch.cat(user_feats, dim=1) if user_feats else None\n",
    "        book_x = torch.cat(book_feats, dim=1) if book_feats else None\n",
    "\n",
    "        return user_x, book_x\n",
    "\n",
    "    def build(self, data):\n",
    "        edge_index = self.build_edges(data)\n",
    "        user_x, book_x = self.build_nodes_features(data)\n",
    "        \n",
    "        graph_data = HeteroData()\n",
    "        graph_data['user'].x = user_x\n",
    "        graph_data['book'].x = book_x\n",
    "        graph_data[('user', 'interact', 'book')].edge_index = edge_index\n",
    "        graph_data = T.ToUndirected()(graph_data) # Convert to undirected\n",
    "        return graph_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0bb194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial size. Users: 43807, Books: 236065, Total: 279872, Edges: 1134132, Density: 0.010967%\n",
      "Filtered size. Users: 3305, Books: 3799, Total: 7104, Edges: 164377, Density: 1.309183%\n",
      "Initial textual embeddings size. Descriptions: 3799, Reviews: 32239, Total: 36038\n",
      "Filtered textual embeddings size. Descriptions: 3799, Reviews: 30733, Total: 34532\n"
     ]
    }
   ],
   "source": [
    "## Scratch pipeline\n",
    "graph_data_loader = GraphDataLoader(cfg)\n",
    "text_embedding_processor = TextEmbeddingProcessor(cfg)\n",
    "\n",
    "data = graph_data_loader.load()\n",
    "data = graph_data_loader.preprocess(data, method='review_coreness')\n",
    "data = text_embedding_processor.to_torch(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e037628d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3799, 384]), torch.Size([30733, 384]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embeddings_descriptions'].shape, data['embeddings_reviews'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fdd8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = data['books_des_id']\n",
    "embeddings = data['embeddings_descriptions']\n",
    "\n",
    "sorted_index, perm = torch.sort(ids)\n",
    "\n",
    "embeddings = embeddings[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee0036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "51ad892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3305, 384])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = data[f'users_review_id']\n",
    "\n",
    "embeddings_reviews = data['embeddings_reviews']\n",
    "\n",
    "embeddings_reviews_aggr = scatter_mean(embeddings_reviews, ids, dim=0)\n",
    "embeddings_reviews_aggr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d68be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_loader = GraphLoader(cfg)\n",
    "\n",
    "graph_data = graph_loader.build(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1f43ad50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  user={ x=[3305, 385] },\n",
       "  book={ x=[3799, 769] },\n",
       "  (user, interact, book)={ edge_index=[2, 164377] },\n",
       "  (book, rev_interact, user)={ edge_index=[2, 164377] }\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aace682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def one_strategy(data):\n",
    "#     \"One strategy for generating graph node features.\"\n",
    "#     # Topological Features\n",
    "#     users_topo_feature, books_topo_feature = graph_node_topological_features(data, feature=[\"degree\"], degree_log_transform=True)\n",
    "    \n",
    "#     # Textual Features\n",
    "#     embeddings_reviews, embeddings_descriptions = embeddings_parquet2torch(data)\n",
    "#     books_textual_feature = graph_node_textual_feature_description(data, embeddings_descriptions)\n",
    "#     user_review_embeddings_aggr, book_review_embeddings_aggr = graph_node_textual_features_reviews_aggr(data, embeddings_reviews, aggr_fn='mean')\n",
    "\n",
    "#     user_x = torch.cat([users_topo_feature, user_review_embeddings_aggr], dim=1)\n",
    "#     book_x = torch.cat([books_topo_feature, books_textual_feature, book_review_embeddings_aggr], dim=1)\n",
    "\n",
    "#     return user_x, book_x\n",
    "\n",
    "# def second_strategy(data, dimensions):\n",
    "#     # Topological Features\n",
    "#     users_topo_feature, books_topo_feature = graph_node_topological_features(data, feature=[\"degree\"], degree_log_transform=True)\n",
    "\n",
    "#     # Random Features\n",
    "#     user_n = data['users'].shape[0]\n",
    "#     book_n = data['books'].shape[0]\n",
    "#     users_random_feature = torch.randn(user_n, dimensions)\n",
    "#     books_random_feature = torch.randn(book_n, dimensions)\n",
    "\n",
    "#     user_x = torch.cat([users_topo_feature, users_random_feature], dim=1)\n",
    "#     book_x = torch.cat([books_topo_feature, books_random_feature], dim=1)\n",
    "\n",
    "#     return user_x, book_x\n",
    "\n",
    "# def third_strategy(data, dimensions):\n",
    "#     user_n = data['users'].shape[0]\n",
    "#     book_n = data['books'].shape[0]\n",
    "#     user_x = torch.randn(user_n, dimensions)\n",
    "#     book_x = torch.randn(book_n, dimensions)\n",
    "\n",
    "#     return user_x, book_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41e6e85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1755874761792,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "f41e6e85",
    "outputId": "3da489ee-72d6-40c2-f02c-3e0a3a5c38ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  user={ x=[3305, 1] },\n",
      "  item={ x=[3799, 1] },\n",
      "  (user, interacts, item)={ edge_index=[2, 164377] },\n",
      "  (item, rev_interacts, user)={ edge_index=[2, 164377] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def igraph_to_hetero_pyg(g, user_type=0, book_type=1, user_node_type='user', item_node_type='item', edge_type='interacts'):\n",
    "    \"\"\"\n",
    "    Convert an igraph bipartite graph to a PyG HeteroData object.\n",
    "    Assumes:\n",
    "      - Vertex attribute 'type' distinguishes users (user_type) and books (book_type)\n",
    "      - Edges are user->book (bipartite)\n",
    "    \"\"\"\n",
    "    # Map igraph node indices to user/item indices\n",
    "    user_indices = [v.index for v in g.vs if v['type'] == user_type]\n",
    "    book_indices = [v.index for v in g.vs if v['type'] == book_type]\n",
    "    user_map = {idx: i for i, idx in enumerate(user_indices)}\n",
    "    book_map = {idx: i for i, idx in enumerate(book_indices)}\n",
    "\n",
    "    # Node features (optional: here just zeros)\n",
    "    user_x = torch.zeros((len(user_indices), 1), dtype=torch.float)\n",
    "    book_x = torch.zeros((len(book_indices), 1), dtype=torch.float)\n",
    "\n",
    "    # Edges: only user->book\n",
    "    src, dst = [], []\n",
    "    for e in g.es:\n",
    "        s, t = e.source, e.target\n",
    "        if g.vs[s]['type'] == user_type and g.vs[t]['type'] == book_type:\n",
    "            src.append(user_map[s])\n",
    "            dst.append(book_map[t])\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "\n",
    "    data = HeteroData()\n",
    "    data[user_node_type].x = user_x\n",
    "    data[item_node_type].x = book_x\n",
    "    data[(user_node_type, edge_type, item_node_type)].edge_index = edge_index\n",
    "    data = T.ToUndirected()(data) # Convert to undirected\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "data = igraph_to_hetero_pyg(subg)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebb66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_node_feature_embedding(data, hidden_channel):\n",
    "    user_embedding = nn.Embedding(data[\"user\"].num_nodes, hidden_channel)\n",
    "    item_embedding = nn.Embedding(data[\"item\"].num_nodes, hidden_channel)\n",
    "    return user_embedding, item_embedding\n",
    "\n",
    "hidden_channel = 64\n",
    "\n",
    "user_embedding, item_embedding = random_node_feature_embedding(data, hidden_channel)\n",
    "\n",
    "data['user'].x = user_embedding.weight\n",
    "data['item'].x = item_embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7afd99c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(30, 64).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f56d3b",
   "metadata": {
    "id": "68f56d3b",
    "outputId": "fe11545f-55c2-46aa-ba5d-1899ac4b5214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[150655, 5], edge_index=[2, 1134132], edge_attr=[1134132, 1])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch_geometric.data import Data\n",
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# def read_graphml_to_hetero_pyg(path, user_prefix=\"u\", item_prefix=\"i\"):\n",
    "#     tree = ET.parse(path)\n",
    "#     root = tree.getroot()\n",
    "#     ns = {'g': 'http://graphml.graphdrawing.org/xmlns'}\n",
    "\n",
    "#     key_map = {}\n",
    "#     for key in root.findall('g:key', ns):\n",
    "#         key_id = key.attrib['id']\n",
    "#         key_map[key_id] = key.attrib.get('attr.name', key_id)\n",
    "\n",
    "#     # Separate users and items by prefix\n",
    "#     user_map, item_map = {}, {}\n",
    "#     user_features, item_features = [], []\n",
    "#     for idx, node in enumerate(root.findall('.//g:node', ns)):\n",
    "#         node_id = node.attrib['id']\n",
    "#         feat = []\n",
    "#         for data in node.findall('g:data', ns):\n",
    "#             value = data.text\n",
    "#             try:\n",
    "#                 feat.append(float(value))\n",
    "#             except (ValueError, TypeError):\n",
    "#                 pass\n",
    "#         feat = feat or [0.0]\n",
    "#         if node_id.startswith(user_prefix):\n",
    "#             user_map[node_id] = len(user_map)\n",
    "#             user_features.append(feat)\n",
    "#         elif node_id.startswith(item_prefix):\n",
    "#             item_map[node_id] = len(item_map)\n",
    "#             item_features.append(feat)\n",
    "\n",
    "#     # Edges: from users to items\n",
    "#     edge_index = [[], []]\n",
    "#     edge_attrs = []\n",
    "#     for edge in root.findall('.//g:edge', ns):\n",
    "#         src_id = edge.attrib['source']\n",
    "#         tgt_id = edge.attrib['target']\n",
    "#         # Only keep user->item edges\n",
    "#         if src_id in user_map and tgt_id in item_map:\n",
    "#             src = user_map[src_id]\n",
    "#             tgt = item_map[tgt_id]\n",
    "#             edge_index[0].append(src)\n",
    "#             edge_index[1].append(tgt)\n",
    "#             feat = []\n",
    "#             for data in edge.findall('g:data', ns):\n",
    "#                 value = data.text\n",
    "#                 try:\n",
    "#                     feat.append(float(value))\n",
    "#                 except (ValueError, TypeError):\n",
    "#                     pass\n",
    "#             edge_attrs.append(feat or [0.0])\n",
    "\n",
    "#     # Convert to tensors\n",
    "#     user_x = torch.tensor(user_features, dtype=torch.float) if user_features else torch.empty((0, 1))\n",
    "#     item_x = torch.tensor(item_features, dtype=torch.float) if item_features else torch.empty((0, 1))\n",
    "#     edge_index = torch.tensor(edge_index, dtype=torch.long) if edge_index[0] else torch.empty((2, 0), dtype=torch.long)\n",
    "#     edge_attr = torch.tensor(edge_attrs, dtype=torch.float) if edge_attrs else torch.empty((0, 1))\n",
    "\n",
    "#     data = HeteroData()\n",
    "#     data['user'].x = user_x\n",
    "#     data['book'].x = item_x\n",
    "#     data['user', 'interacts', 'book'].edge_index = edge_index\n",
    "#     data['user', 'interacts', 'book'].edge_attr = edge_attr\n",
    "#     return data\n",
    "\n",
    "# # Example usage\n",
    "# DIR = 'data_sample'\n",
    "# data = read_graphml_to_pyg(os.path.join(DIR, \"graph.graphml\"))\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e485a421",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1755874770394,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "e485a421",
    "outputId": "b05f6b9d-9b8c-44c4-8036-5821e8797d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "==============\n",
      "HeteroData(\n",
      "  user={ x=[3305, 64] },\n",
      "  item={ x=[3799, 64] },\n",
      "  (user, interacts, item)={\n",
      "    edge_index=[2, 131503],\n",
      "    edge_label=[131503],\n",
      "    edge_label_index=[2, 131503],\n",
      "  },\n",
      "  (item, rev_interacts, user)={ edge_index=[2, 131503] }\n",
      ")\n",
      "\n",
      "Validation data:\n",
      "================\n",
      "HeteroData(\n",
      "  user={ x=[3305, 64] },\n",
      "  item={ x=[3799, 64] },\n",
      "  (user, interacts, item)={\n",
      "    edge_index=[2, 131503],\n",
      "    edge_label=[49311],\n",
      "    edge_label_index=[2, 49311],\n",
      "  },\n",
      "  (item, rev_interacts, user)={ edge_index=[2, 131503] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## edge_index : edges used for the message passing\n",
    "## edge_label : edges used for the supervised learning task.\n",
    "## Some edges in edge_label_index might be in edge_index, as they might be used for supervision and message passing\n",
    "\n",
    "# % of edges used for supervision that are not in the message passing (theoretically)\n",
    "disjoint_train_ratio = 0.0 # We set it zero for transductive learning\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    disjoint_train_ratio=disjoint_train_ratio, \n",
    "    neg_sampling_ratio=2.0,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"user\", \"interacts\", \"item\"),\n",
    "    rev_edge_types=(\"item\", \"rev_interacts\", \"user\"),\n",
    ")\n",
    "train_data, val_data, test_data = transform(data)\n",
    "print(\"Training data:\")\n",
    "print(\"==============\")\n",
    "print(train_data)\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"================\")\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76877c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Overlapping edges found between edge_index and edge_label_index\n",
      "Overlapping edges size: 131503\n"
     ]
    }
   ],
   "source": [
    "def validate_edge_indices(data):\n",
    "    all_index = set()\n",
    "    edge_index = data[\"user\", \"interacts\", \"item\"].edge_index\n",
    "    for u, v in zip(edge_index[0], edge_index[1]):\n",
    "        all_index.add((u.item(), v.item()))\n",
    "    \n",
    "    all_label_index = set()\n",
    "    edge_label_index = data[\"user\", \"interacts\", \"item\"].edge_label_index\n",
    "    for u, v in zip(edge_label_index[0], edge_label_index[1]):\n",
    "        all_label_index.add((u.item(), v.item()))\n",
    "\n",
    "    assert len(all_index) == len(edge_index[0])\n",
    "    assert len(all_label_index) == len(edge_label_index[0])\n",
    "\n",
    "    intersection = all_index.intersection(all_label_index)\n",
    "    \n",
    "\n",
    "    if len(intersection) > 0:\n",
    "        print(f\"Warning: Overlapping edges found between edge_index and edge_label_index\")\n",
    "        print(f\"Overlapping edges size: {len(intersection)}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "validate_edge_indices(train_data)\n",
    "validate_edge_indices(val_data)\n",
    "validate_edge_indices(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906be191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 612, 1065,    8,  ...,  603, 1608, 2245],\n",
       "        [2017, 3440, 1608,  ..., 3726, 3680,  343]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"user\", \"interacts\", \"item\"].edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f472c1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "error",
     "timestamp": 1755874773128,
     "user": {
      "displayName": "angelo cao",
      "userId": "13059316039091548203"
     },
     "user_tz": -120
    },
    "id": "8f472c1b",
    "outputId": "a6f92989-8db3-4ee4-be17-b4966487708d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled mini-batch:\n",
      "===================\n",
      "HeteroData(\n",
      "  user={\n",
      "    x=[2803, 64],\n",
      "    n_id=[2803],\n",
      "  },\n",
      "  item={\n",
      "    x=[3018, 64],\n",
      "    n_id=[3018],\n",
      "  },\n",
      "  (user, interacts, item)={\n",
      "    edge_index=[2, 13499],\n",
      "    edge_label=[128],\n",
      "    edge_label_index=[2, 128],\n",
      "    e_id=[13499],\n",
      "    input_id=[64],\n",
      "  },\n",
      "  (item, rev_interacts, user)={\n",
      "    edge_index=[2, 14228],\n",
      "    e_id=[14228],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 64\n",
    "num_negative_sampling_ratio = 1.0\n",
    "\n",
    "# Define seed edges:\n",
    "edge_label_index = train_data[\"user\", \"interacts\", \"item\"].edge_label_index\n",
    "edge_label = train_data[\"user\", \"interacts\", \"item\"].edge_label\n",
    "\n",
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    edge_label_index=((\"user\", \"interacts\", \"item\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=batch_size,\n",
    "    neg_sampling=NegativeSampling(mode='binary', amount=num_negative_sampling_ratio), # Let's first use the default option, we will improve it later if needed\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Inspect a sample:\n",
    "sampled_data = next(iter(train_loader))\n",
    "\n",
    "print(\"Sampled mini-batch:\")\n",
    "print(\"===================\")\n",
    "print(sampled_data)\n",
    "\n",
    "assert sampled_data[\"user\", \"interacts\", \"item\"].edge_label_index.size(1) == (num_negative_sampling_ratio + 1) * batch_size\n",
    "assert sampled_data[\"user\", \"interacts\", \"item\"].edge_label.min() == 0\n",
    "assert sampled_data[\"user\", \"interacts\", \"item\"].edge_label.max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109bde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    def __init__(self, \n",
    "                 encoder, \n",
    "                 decoder,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        gnn_layer_cls: class of the GNN layer (e.g., GCNConv, SAGEConv)\n",
    "        encoder_cls: class of the encoder (should accept gnn_layer_cls, in_channels, hidden_channels, latent_dim)\n",
    "        decoder_cls: class of the decoder (should accept no arguments)\n",
    "        in_channels: input feature dimension\n",
    "        hidden_channels: hidden layer dimension\n",
    "        latent_dim: latent space dimension\n",
    "        encoder_kwargs: additional kwargs for encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        mu, logvar = self.encoder(x, edge_index)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index):\n",
    "        pos_score = torch.sigmoid(self.decode(z, pos_edge_index))\n",
    "        neg_score = torch.sigmoid(self.decode(z, neg_edge_index))\n",
    "        pos_loss = -torch.log(pos_score + 1e-15).mean()\n",
    "        neg_loss = -torch.log(1 - neg_score + 1e-15).mean()\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def kl_loss(self, mu, logvar):\n",
    "        kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return kl_div\n",
    "\n",
    "class VGAEncoder(nn.Module):\n",
    "    def __init__(self, GNNLayer, in_channels, hidden_channels, latent_dim):\n",
    "        \"\"\"\n",
    "        in_channels: dimension of the node input features (after projecting items and using user embeddings)\n",
    "        hidden_channels: hidden units for first GCN layer\n",
    "        latent_dim: dimension of mu and logvar\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GNNLayer(in_channels, hidden_channels)\n",
    "        self.conv_mu = GNNLayer(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GNNLayer(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z_user, z_book, edge_index, sigmoid=True):\n",
    "        edge_feat_user = z_user[edge_index[0]]\n",
    "        edge_feat_book = z_book[edge_index[1]]\n",
    "        score = (edge_feat_user * edge_feat_book).sum(dim=-1)\n",
    "        if sigmoid:\n",
    "            score = torch.sigmoid(score)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdbe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch training loop using train_loader and sampled_data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate model\n",
    "in_channels = data['user'].x.shape[1]\n",
    "hidden_channels = 32\n",
    "latent_dim = 16\n",
    "\n",
    "encoder = VGAEncoder(GCNConv, in_channels, hidden_channels, latent_dim).to(device)\n",
    "decoder = InnerProductDecoder().to(device)\n",
    "model = VGAE(encoder, decoder).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Mini-batch training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm.tqdm(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        user_x = batch['user'].x\n",
    "        book_x = batch['book'].x\n",
    "        x = torch.cat([user_x, book_x], dim=0)\n",
    "        edge_index = batch['user', 'interacts', 'book'].edge_index\n",
    "\n",
    "        # Encode and decode\n",
    "        z, mu, logvar = model(x, edge_index)\n",
    "\n",
    "        # Use edge_label_index for positive edges, sample negatives\n",
    "        pos_edge_index = batch['user', 'interacts', 'book'].edge_label_index\n",
    "        # Negative sampling: random pairs (for demonstration)\n",
    "        num_neg = pos_edge_index.shape[1]\n",
    "        neg_src = torch.randint(0, x.size(0), (num_neg,), device=device)\n",
    "        neg_dst = torch.randint(0, batch['book'].x.size(0), (num_neg,), device=device)\n",
    "        neg_edge_index = torch.stack([neg_src, neg_dst], dim=0)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_recon = model.recon_loss(z, pos_edge_index, neg_edge_index)\n",
    "        loss_kl = model.kl_loss(mu, logvar)\n",
    "        loss = loss_recon + loss_kl\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac83e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 gnn_layer_cls,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 user_in_channels,\n",
    "                 item_in_channels,\n",
    "                 hidden_channels,\n",
    "                 latent_dim,\n",
    "                 emb_linear_transform,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        gnn_layer_cls: class of the GNN layer (e.g., GCNConv, SAGEConv)\n",
    "        encoder_cls: class of the encoder (should accept gnn_layer_cls, in_channels, hidden_channels, latent_dim)\n",
    "        decoder_cls: class of the decoder (should accept no arguments)\n",
    "        in_channels: input feature dimension\n",
    "        hidden_channels: hidden layer dimension\n",
    "        latent_dim: latent space dimension\n",
    "        encoder_kwargs: additional kwargs for encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_linear_transform = emb_linear_transform\n",
    "        self.user_linear = nn.Linear(user_in_channels, hidden_channels)\n",
    "        self.item_linear = nn.Linear(item_in_channels, hidden_channels)\n",
    "\n",
    "        if user_in_channels != item_in_channels:\n",
    "            print(f\"Inconsistent input feature dimensions: user={user_in_channels}, item={item_in_channels}\")\n",
    "            print(f\"Forcing linear transformation\")\n",
    "            self.emb_linear_transform = True\n",
    "\n",
    "        self.input_channels = hidden_channels if self.emb_linear_transform else user_in_channels\n",
    "\n",
    "        # Random initialization of embeddings, don't use it for now\n",
    "        # self.user_embedding = nn.Embedding(data[user].num_nodes, hidden_channels)\n",
    "        # self.item_embedding = nn.Embedding(data[item].num_nodes, hidden_channels)\n",
    "\n",
    "        self.encoder = encoder(gnn_layer_cls, hidden_channels, hidden_channels, latent_dim)\n",
    "        self.encoder = to_hetero(self.encoder, metadata=data.metadata())\n",
    "        self.decoder = decoder()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, data):\n",
    "        if self.emb_linear_transform:\n",
    "            x_dict = {\n",
    "                \"user\": self.user_linear(data[\"user\"].x),\n",
    "                \"item\": self.item_linear(data[\"item\"].x)\n",
    "            }\n",
    "        else:\n",
    "            x_dict = {\n",
    "                \"user\": data[\"user\"].x,\n",
    "                \"item\": data[\"item\"].x\n",
    "            }\n",
    "\n",
    "        mu, logvar = self.encoder(x_dict, data.edge_index_dict)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index):\n",
    "        pos_score = self.decode(z, pos_edge_index)\n",
    "        neg_score = self.decode(z, neg_edge_index)\n",
    "        pos_loss = -torch.log(pos_score + 1e-15).mean()\n",
    "        neg_loss = -torch.log(1 - neg_score + 1e-15).mean()\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def kl_loss(self, mu, logvar):\n",
    "        kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return kl_div\n",
    "\n",
    "class VGAEncoder(nn.Module):\n",
    "    def __init__(self, GNNLayer, in_channels, hidden_channels, latent_dim):\n",
    "        \"\"\"\n",
    "        in_channels: dimension of the node input features (after projecting items and using user embeddings)\n",
    "        hidden_channels: hidden units for first GCN layer\n",
    "        latent_dim: dimension of mu and logvar\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GNNLayer(in_channels, hidden_channels)\n",
    "        self.conv_mu = GNNLayer(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GNNLayer(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z_user, z_item, edge_index, sigmoid=True):\n",
    "        edge_feat_user = z_user[edge_index[0]]\n",
    "        edge_feat_item = z_item[edge_index[1]]\n",
    "        score = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "        if sigmoid:\n",
    "            score = torch.sigmoid(score)\n",
    "        return score\n",
    "\n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f9ab44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 gnn_layer_cls,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 user_in_channels,\n",
    "                 item_in_channels,\n",
    "                 hidden_channels,\n",
    "                 latent_dim,\n",
    "                 emb_linear_transform,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        gnn_layer_cls: class of the GNN layer (e.g., GCNConv, SAGEConv)\n",
    "        encoder_cls: class of the encoder (should accept gnn_layer_cls, in_channels, hidden_channels, latent_dim)\n",
    "        decoder_cls: class of the decoder (should accept no arguments)\n",
    "        in_channels: input feature dimension\n",
    "        hidden_channels: hidden layer dimension\n",
    "        latent_dim: latent space dimension\n",
    "        encoder_kwargs: additional kwargs for encoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_linear_transform = emb_linear_transform\n",
    "        self.user_linear = nn.Linear(user_in_channels, hidden_channels)\n",
    "        self.item_linear = nn.Linear(item_in_channels, hidden_channels)\n",
    "\n",
    "        if user_in_channels != item_in_channels:\n",
    "            print(f\"Inconsistent input feature dimensions: user={user_in_channels}, item={item_in_channels}\")\n",
    "            print(f\"Forcing linear transformation\")\n",
    "            self.emb_linear_transform = True\n",
    "\n",
    "        input_channels = hidden_channels if self.emb_linear_transform else user_in_channels\n",
    "\n",
    "        # Random initialization of embeddings, don't use it for now\n",
    "        # self.user_embedding = nn.Embedding(data[user].num_nodes, hidden_channels)\n",
    "        # self.item_embedding = nn.Embedding(data[item].num_nodes, hidden_channels)\n",
    "\n",
    "        self.encoder = encoder(gnn_layer_cls, input_channels, hidden_channels, latent_dim)\n",
    "        self.decoder = decoder()\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, data):\n",
    "        if self.emb_linear_transform:\n",
    "            user_x = self.user_linear(data[\"user\"].x)\n",
    "            item_x = self.item_linear(data[\"item\"].x)\n",
    "        else:\n",
    "            user_x = data[\"user\"].x\n",
    "            item_x = data[\"item\"].x\n",
    "\n",
    "        x = torch.cat([user_x, item_x], dim=0)\n",
    "        edge_index = data['user', 'interacts', 'item'].edge_index\n",
    "\n",
    "        mu, logvar = self.encoder(x, edge_index)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return self.decoder(z, edge_index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VGAEncoder(nn.Module):\n",
    "    def __init__(self, GNNLayer, in_channels, hidden_channels, latent_dim):\n",
    "        \"\"\"\n",
    "        in_channels: dimension of the node input features (after projecting items and using user embeddings)\n",
    "        hidden_channels: hidden units for first GCN layer\n",
    "        latent_dim: dimension of mu and logvar\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GNNLayer(in_channels, hidden_channels)\n",
    "        self.conv_mu = GNNLayer(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GNNLayer(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.leaky_relu(x)\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "class InnerProductDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        edge_feat_user = z[edge_index[0]]\n",
    "        edge_feat_item = z[edge_index[1]]\n",
    "        score = (edge_feat_user * edge_feat_item).sum(dim=-1)\n",
    "        if sigmoid:\n",
    "            score = torch.sigmoid(score)\n",
    "        return score\n",
    "    \n",
    "    def forward_all(self, z, sigmoid=True):\n",
    "        \"Use it cautiously, as it computes all pairwise interactions\"\n",
    "        A_pred = (z @ z.T).view(-1)\n",
    "        if sigmoid:\n",
    "            A_pred = torch.sigmoid(A_pred)\n",
    "        return A_pred\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b581173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_recon_loss(z, preds, batch):\n",
    "    edge_labels = batch[\"user\", \"interacts\", \"item\"].edge_label\n",
    "    pos_mask = edge_labels == 1\n",
    "    pos_score = preds[pos_mask]\n",
    "    neg_score = preds[~pos_mask]\n",
    "\n",
    "    pos_loss = -torch.log(pos_score + 1e-15).mean()\n",
    "    neg_loss = -torch.log(1 - neg_score + 1e-15).mean()\n",
    "\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "def kl_loss(mu, logvar):\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdbc2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import AveragePrecision, AUROC\n",
    "\n",
    "def compute_auc(y_scores, y_true):\n",
    "    y_true = y_true.long() \n",
    "    score = AUROC(task=\"binary\")(y_scores, y_true).item()\n",
    "    return score\n",
    "\n",
    "def compute_average_precision(y_scores, y_true):\n",
    "    y_true = y_true.long() \n",
    "    score = AveragePrecision(task=\"binary\")(y_scores, y_true).item()\n",
    "    return score\n",
    "\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_loss_recon = 0\n",
    "    total_loss_kl = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            z, mu, logvar = model(batch)\n",
    "            edge_labels = batch[\"user\", \"interacts\", \"item\"].edge_label\n",
    "            edge_index = batch[\"user\", \"interacts\", \"item\"].edge_label_index\n",
    "            preds = model.decode(z, edge_index)\n",
    "            all_labels.append(edge_labels)\n",
    "            all_preds.append(preds)\n",
    "\n",
    "            loss_recon = binary_recon_loss(z, preds, batch)\n",
    "            loss_kl = kl_loss(mu, logvar)\n",
    "\n",
    "            loss = loss_recon + loss_kl\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_loss_recon += loss_recon.item()\n",
    "            total_loss_kl += loss_kl.item()\n",
    "\n",
    "    y_scores = torch.cat(all_preds)\n",
    "    y_true = torch.cat(all_labels)\n",
    "    auc = compute_auc(y_scores, y_true)\n",
    "    avg_precision = compute_average_precision(y_scores, y_true)\n",
    "    return { \"total_loss\": total_loss, \n",
    "             \"total_loss_recon\": total_loss_recon, \n",
    "             \"total_loss_kl\": total_loss_kl, \n",
    "             \"auc\": auc, \n",
    "             \"average_precision\": avg_precision }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "438cedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3 [00:00<?, ?Epochs/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3 [06:32<?, ?Epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Epoch 0, Total Loss: 1.6783, Rec. Loss: 1.4924, KL Loss: 0.1860\n",
      "AUC: 0.5130, Average Precision: 0.5057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# VALIDATION DATA\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m evaluate(\u001b[43mval_loader\u001b[49m, model)\n\u001b[0;32m    103\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    104\u001b[0m history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecon_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss_recon\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop for your new VGAE version (from cell 17)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gnn_layer_cls = GCNConv\n",
    "user_in_channels = train_data[\"user\"].num_features\n",
    "item_in_channels = train_data[\"item\"].num_features\n",
    "\n",
    "## Hyperparameters\n",
    "hidden_channels = 32\n",
    "latent_dim = 16\n",
    "\n",
    "# Instantiate model\n",
    "model = VGAE(\n",
    "    gnn_layer_cls=gnn_layer_cls,\n",
    "    encoder=VGAEncoder,\n",
    "    decoder=InnerProductDecoder,\n",
    "    user_in_channels=user_in_channels,\n",
    "    item_in_channels=item_in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    latent_dim=latent_dim,\n",
    "    emb_linear_transform=False\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "history = {\n",
    "    \"train\" : defaultdict(list),\n",
    "    \"val\" : defaultdict(list),\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in trange(epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "    total_loss = 0\n",
    "    total_loss_recon = 0\n",
    "    total_loss_kl = 0\n",
    "    model.train()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        z, mu, logvar = model(batch)\n",
    "        edge_index = batch[\"user\", \"interacts\", \"item\"].edge_label_index\n",
    "        edge_labels = batch[\"user\", \"interacts\", \"item\"].edge_label\n",
    "        preds = model.decode(z, edge_index)\n",
    "\n",
    "        ## Store the prediction\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(edge_labels)\n",
    "\n",
    "        ## Compute the loss\n",
    "        loss_recon = binary_recon_loss(z, preds, batch)\n",
    "        loss_kl = kl_loss(mu, logvar)\n",
    "        loss = loss_recon + loss_kl\n",
    "\n",
    "        # Store the loss\n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(edge_labels)\n",
    "\n",
    "        total_loss_recon += loss_recon.item()\n",
    "        total_loss_kl += loss_kl.item()\n",
    "        \n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    ## Evaluation\n",
    "    total_loss /= len(train_loader)\n",
    "    total_loss_recon /= len(train_loader)\n",
    "    total_loss_kl /= len(train_loader)\n",
    "    history[\"train\"][\"total_loss\"].append(total_loss)\n",
    "    history[\"train\"][\"recon_loss\"].append(total_loss_recon)\n",
    "    history[\"train\"][\"kl_loss\"].append(total_loss_kl)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        # TRAINING DATA\n",
    "        y_scores, y_true = torch.cat(all_preds), torch.cat(all_labels)\n",
    "        auc_score = compute_auc(y_scores, y_true)\n",
    "        avg_precision = compute_average_precision(y_scores, y_true)\n",
    "        history[\"train\"][\"auc\"].append(auc_score)\n",
    "        history[\"train\"][\"average_precision\"].append(avg_precision)\n",
    "\n",
    "        print(f\"TRAINING DATA\")\n",
    "        print(f\"Epoch {epoch}, Total Loss: {total_loss:.4f}, Rec. Loss: {total_loss_recon:.4f}, KL Loss: {total_loss_kl:.4f}\")\n",
    "        print(f\"AUC: {auc_score:.4f}, Average Precision: {avg_precision:.4f}\")\n",
    "\n",
    "        # VALIDATION DATA\n",
    "        val_metrics = evaluate(val_loader, model)\n",
    "        history[\"val\"][\"total_loss\"].append(val_metrics[\"total_loss\"])\n",
    "        history[\"val\"][\"recon_loss\"].append(val_metrics[\"total_loss_recon\"])\n",
    "        history[\"val\"][\"kl_loss\"].append(val_metrics[\"total_loss_kl\"])\n",
    "        history[\"val\"][\"auc\"].append(val_metrics[\"auc\"])\n",
    "        history[\"val\"][\"average_precision\"].append(val_metrics[\"average_precision\"])\n",
    "\n",
    "        print(f\"VALIDATION DATA\")\n",
    "        print(f\"Epoch {epoch}, Total Loss: {val_metrics['total_loss']:.4f}, Rec. Loss: {val_metrics['total_loss_recon']:.4f}, KL Loss: {val_metrics['total_loss_kl']:.4f}\")\n",
    "        print(f\"AUC: {val_metrics['auc']:.4f}, Average Precision: {val_metrics['average_precision']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77682c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NegativeSampling:\n",
    "#     def __init__(self, num_samples):\n",
    "#         self.num_samples = num_samples\n",
    "\n",
    "#     def sample(self, pos_edge_index, num_nodes):\n",
    "#         neg_src = torch.randint(0, num_nodes, (self.num_samples,))\n",
    "#         neg_dst = torch.randint(0, num_nodes, (self.num_samples,))\n",
    "#         neg_edge_index = torch.stack([neg_src, neg_dst], dim=0)\n",
    "#         return neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d1f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e56305",
   "metadata": {
    "id": "c6e56305"
   },
   "outputs": [],
   "source": [
    "class HybridBPRRatingAutoencoder(nn.Module):\n",
    "    def __init__(self, num_features_u, num_features_v, hidden_dim, latent_dim, rating_range=(1, 5)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rating_min, self.rating_max = rating_range\n",
    "\n",
    "        # Encoders (same as before)\n",
    "        self.encoder_u = nn.Sequential(\n",
    "            GCNConv(num_features_u, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            GCNConv(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.encoder_v = nn.Sequential(\n",
    "            GCNConv(num_features_v, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            GCNConv(hidden_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "        # Rating prediction head\n",
    "        self.rating_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x_u, x_v, edge_index):\n",
    "        z_u = self.encoder_u(x_u, edge_index)\n",
    "        z_v = self.encoder_v(x_v, edge_index)\n",
    "        return z_u, z_v\n",
    "\n",
    "    def predict_score(self, z_u, z_v, user_ids, item_ids):\n",
    "        \"\"\"For BPR ranking\"\"\"\n",
    "        user_emb = z_u[user_ids]\n",
    "        item_emb = z_v[item_ids]\n",
    "        return torch.sum(user_emb * item_emb, dim=1)\n",
    "\n",
    "    def predict_rating(self, z_u, z_v, user_ids, item_ids):\n",
    "        \"\"\"For explicit rating prediction\"\"\"\n",
    "        user_emb = z_u[user_ids]\n",
    "        item_emb = z_v[item_ids]\n",
    "        edge_embeddings = torch.cat([user_emb, item_emb], dim=1)\n",
    "\n",
    "        raw_ratings = self.rating_decoder(edge_embeddings)\n",
    "        ratings = raw_ratings * (self.rating_max - self.rating_min) + self.rating_min\n",
    "\n",
    "        return ratings.squeeze()\n",
    "\n",
    "    def forward(self, x_u, x_v, edge_index):\n",
    "        z_u, z_v = self.encode(x_u, x_v, edge_index)\n",
    "        return z_u, z_v\n",
    "\n",
    "def hybrid_loss(model, z_u, z_v,\n",
    "                # For explicit ratings\n",
    "                rating_user_ids, rating_item_ids, true_ratings, rating_mask,\n",
    "                # For BPR\n",
    "                bpr_user_ids, bpr_pos_items, bpr_neg_items,\n",
    "                rating_weight=1.0, bpr_weight=0.1):\n",
    "    \"\"\"\n",
    "    Combines rating prediction loss and BPR loss\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "\n",
    "    # Rating loss (only on observed ratings)\n",
    "    if rating_mask.sum() > 0:\n",
    "        observed_idx = rating_mask.bool()\n",
    "        pred_ratings = model.predict_rating(z_u, z_v,\n",
    "                                           rating_user_ids[observed_idx],\n",
    "                                           rating_item_ids[observed_idx])\n",
    "        rating_loss = F.mse_loss(pred_ratings, true_ratings[observed_idx])\n",
    "        total_loss += rating_weight * rating_loss\n",
    "\n",
    "    # BPR loss for ranking\n",
    "    if len(bpr_user_ids) > 0:\n",
    "        bpr_loss_val = bpr_loss(model, z_u, z_v, bpr_user_ids, bpr_pos_items, bpr_neg_items)\n",
    "        total_loss += bpr_weight * bpr_loss_val\n",
    "\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
